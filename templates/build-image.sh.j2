#jinja2: trim_blocks: True, lstrip_blocks: True
#!/usr/bin/env bash
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

set -xe

SELF_DIR="$(cd "$(dirname "$0")"; pwd)"

source "${SELF_DIR}/.env"

# use fast copy (CoW)
# https://github.com/pkolaczk/fclones/issues/219
CP="cp --reflink=auto"
if [ "$(uname)" == "Darwin" ]; then
  CP="cp -c"
fi

mkdir -p base-ubuntu-2004/download
if [ $(uname -m) = "arm64" ] || [ $(uname -m) = "aarch64" ]; then JDK8_TAR_NAME=zulu${ZULU8_VERSION}-ca-jdk${JDK8_VERSION}-linux_aarch64; else JDK8_TAR_NAME=zulu${ZULU8_VERSION}-ca-jdk${JDK8_VERSION}-linux_x64; fi
$CP download/${JDK8_TAR_NAME}.tar.gz base-ubuntu-2004/download/${JDK8_TAR_NAME}.tar.gz
if [ $(uname -m) = "arm64" ] || [ $(uname -m) = "aarch64" ]; then JDK11_TAR_NAME=zulu${ZULU11_VERSION}-ca-jdk${JDK11_VERSION}-linux_aarch64; else JDK11_TAR_NAME=zulu${ZULU11_VERSION}-ca-jdk${JDK11_VERSION}-linux_x64; fi
$CP download/${JDK11_TAR_NAME}.tar.gz base-ubuntu-2004/download/${JDK11_TAR_NAME}.tar.gz
if [ $(uname -m) = "arm64" ] || [ $(uname -m) = "aarch64" ]; then JDK17_TAR_NAME=zulu${ZULU17_VERSION}-ca-jdk${JDK17_VERSION}-linux_aarch64; else JDK17_TAR_NAME=zulu${ZULU17_VERSION}-ca-jdk${JDK17_VERSION}-linux_x64; fi
$CP download/${JDK17_TAR_NAME}.tar.gz base-ubuntu-2004/download/${JDK17_TAR_NAME}.tar.gz
{% if jdk21_enabled %}
if [ $(uname -m) = "arm64" ] || [ $(uname -m) = "aarch64" ]; then JDK21_TAR_NAME=zulu${ZULU21_VERSION}-ca-jdk${JDK21_VERSION}-linux_aarch64; else JDK21_TAR_NAME=zulu${ZULU21_VERSION}-ca-jdk${JDK21_VERSION}-linux_x64; fi
$CP download/${JDK21_TAR_NAME}.tar.gz base-ubuntu-2004/download/${JDK21_TAR_NAME}.tar.gz
{% endif %}

docker build \
  --file "${SELF_DIR}/base-ubuntu-2004/Dockerfile" \
  --build-arg JDK8_TAR_NAME=${JDK8_TAR_NAME} \
  --build-arg JDK11_TAR_NAME=${JDK11_TAR_NAME} \
  --build-arg JDK17_TAR_NAME=${JDK17_TAR_NAME} \
  --build-arg JDK21_TAR_NAME=${JDK21_TAR_NAME} \
  --tag hadoop-testing/base-ubuntu-2004:${PROJECT_VERSION} \
  "${SELF_DIR}/base-ubuntu-2004" $@

rm -rf base-ubuntu-2004/download/*

{% if kerberos_enabled %}
docker build \
  --build-arg PROJECT_VERSION=${PROJECT_VERSION} \
  --file "${SELF_DIR}/kdc/Dockerfile" \
  --tag hadoop-testing/kdc:${PROJECT_VERSION} \
  "${SELF_DIR}/kdc" $@
{% endif %}

function build_hadoop_master_image() {
  local INDEX=$1
  mkdir -p hadoop-master${INDEX}/download
  {% if zeppelin_enabled %}
  $CP download/zeppelin-${ZEPPELIN_VERSION}-bin{{ '-%s' % zeppelin_custom_name if zeppelin_custom_name }}.tgz hadoop-master${INDEX}/download/zeppelin-${ZEPPELIN_VERSION}-bin{{ '-%s' % zeppelin_custom_name if zeppelin_custom_name }}.tgz
  {% endif %}
  $CP download/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz hadoop-master${INDEX}/download/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz
  if [ $(uname -m) = "arm64" ] || [ $(uname -m) = "aarch64" ]; then HADOOP_TAR_NAME=hadoop-${HADOOP_VERSION}-aarch64; else HADOOP_TAR_NAME=hadoop-${HADOOP_VERSION}; fi
  $CP download/${HADOOP_TAR_NAME}.tar.gz hadoop-master${INDEX}/download/hadoop-${HADOOP_VERSION}.tar.gz
  $CP download/apache-hive-${HIVE_VERSION}-bin.tar.gz hadoop-master${INDEX}/download/apache-hive-${HIVE_VERSION}-bin.tar.gz
  {% if spark_enabled %}
  $CP download/spark-${SPARK_VERSION}-bin-{{ spark_custom_name }}.tgz hadoop-master${INDEX}/download/spark-${SPARK_VERSION}-bin-{{ spark_custom_name }}.tgz
  {% endif %}
  {% if flink_enabled %}
  $CP download/flink-${FLINK_VERSION}-bin-scala_${SCALA_BINARY_VERSION}.tgz hadoop-master${INDEX}/download/flink-${FLINK_VERSION}-bin-scala_${SCALA_BINARY_VERSION}.tgz
  $CP download/flink-sql-connector-hive-${FLINK_HIVE_VERSION}_${SCALA_BINARY_VERSION}-${FLINK_VERSION}.jar hadoop-master${INDEX}/download/flink-sql-connector-hive-${FLINK_HIVE_VERSION}_${SCALA_BINARY_VERSION}-${FLINK_VERSION}.jar
  {% endif %}
  $CP download/apache-kyuubi-${KYUUBI_VERSION}-bin.tgz hadoop-master${INDEX}/download/apache-kyuubi-${KYUUBI_VERSION}-bin.tgz
  $CP download/kyuubi-hive-jdbc-shaded-${KYUUBI_VERSION}.jar hadoop-master${INDEX}/download/kyuubi-hive-jdbc-shaded-${KYUUBI_VERSION}.jar
  {% if ranger_enabled %}
  $CP download/ranger-${RANGER_VERSION}-admin.tar.gz hadoop-master${INDEX}/download/ranger-${RANGER_VERSION}-admin.tar.gz
  {% endif %}
  {% if spark_enabled %}
  $CP download/kyuubi-spark-connector-tpch_${SCALA_BINARY_VERSION}-${KYUUBI_VERSION}.jar hadoop-master${INDEX}/download/kyuubi-spark-connector-tpch_${SCALA_BINARY_VERSION}-${KYUUBI_VERSION}.jar
  $CP download/kyuubi-spark-connector-tpcds_${SCALA_BINARY_VERSION}-${KYUUBI_VERSION}.jar hadoop-master${INDEX}/download/kyuubi-spark-connector-tpcds_${SCALA_BINARY_VERSION}-${KYUUBI_VERSION}.jar
  {% endif %}
  $CP download/mysql-connector-j-${MYSQL_JDBC_VERSION}.jar hadoop-master${INDEX}/download/mysql-connector-j-${MYSQL_JDBC_VERSION}.jar
  $CP download/log4j2-appender-nodep-${LOKI_APPENDER_VERSION}.jar hadoop-master${INDEX}/download/log4j2-appender-nodep-${LOKI_APPENDER_VERSION}.jar
  {% if spark_enabled and iceberg_enabled %}
  $CP download/iceberg-spark-runtime-${SPARK_BINARY_VERSION}_${SCALA_BINARY_VERSION}-${ICEBERG_VERSION}.jar hadoop-master${INDEX}/download/iceberg-spark-runtime-${SPARK_BINARY_VERSION}_${SCALA_BINARY_VERSION}-${ICEBERG_VERSION}.jar
  {% endif %}
  {% if flink_enabled and iceberg_enabled %}
  $CP download/iceberg-flink-runtime-${FLINK_BINARY_VERSION}-${ICEBERG_VERSION}.jar hadoop-master${INDEX}/download/iceberg-flink-runtime-${FLINK_BINARY_VERSION}-${ICEBERG_VERSION}.jar
  {% endif %}
  {% if spark_enabled and hudi_enabled %}
  $CP download/hudi-spark${SPARK_BINARY_VERSION}-bundle_${SCALA_BINARY_VERSION}-${HUDI_VERSION}.jar hadoop-master${INDEX}/download/hudi-spark${SPARK_BINARY_VERSION}-bundle_${SCALA_BINARY_VERSION}-${HUDI_VERSION}.jar
  {% endif %}
  $CP download/jcl-over-slf4j-1.7.36.jar hadoop-master${INDEX}/download/jcl-over-slf4j-1.7.36.jar
  {% if trino_enabled %}
  $CP download/trino-server-${TRINO_VERSION}.tar.gz hadoop-master${INDEX}/download/trino-server-${TRINO_VERSION}.tar.gz
  $CP download/trino-cli-${TRINO_VERSION}-executable.jar hadoop-master${INDEX}/download/trino-cli-${TRINO_VERSION}-executable.jar
  {% endif %}
  {% if parquet_enabled %}
  $CP download/parquet-cli-${PARQUET_VERSION}-runtime.jar hadoop-master${INDEX}/download/parquet-cli-${PARQUET_VERSION}-runtime.jar
  {% endif %}

  docker build \
    --build-arg PROJECT_VERSION=${PROJECT_VERSION} \
    --build-arg ZEPPELIN_VERSION=${ZEPPELIN_VERSION} \
    --build-arg ZOOKEEPER_VERSION=${ZOOKEEPER_VERSION} \
    --build-arg HADOOP_VERSION=${HADOOP_VERSION} \
    --build-arg HIVE_VERSION=${HIVE_VERSION} \
    --build-arg SPARK_VERSION=${SPARK_VERSION} \
    --build-arg SPARK_BINARY_VERSION=${SPARK_BINARY_VERSION} \
    --build-arg FLINK_VERSION=${FLINK_VERSION} \
    --build-arg FLINK_BINARY_VERSION=${FLINK_BINARY_VERSION} \
    --build-arg FLINK_HIVE_VERSION=${FLINK_HIVE_VERSION} \
    --build-arg SCALA_BINARY_VERSION=${SCALA_BINARY_VERSION} \
    --build-arg KYUUBI_VERSION=${KYUUBI_VERSION} \
    --build-arg RANGER_VERSION=${RANGER_VERSION} \
    --build-arg MYSQL_JDBC_VERSION=${MYSQL_JDBC_VERSION} \
    --build-arg ICEBERG_VERSION=${ICEBERG_VERSION} \
    --build-arg HUDI_VERSION=${HUDI_VERSION} \
    --build-arg LOKI_APPENDER_VERSION=${LOKI_APPENDER_VERSION} \
    --build-arg TRINO_VERSION=${TRINO_VERSION} \
    --build-arg PARQUET_VERSION=${PARQUET_VERSION} \
    --file "${SELF_DIR}/hadoop-master${INDEX}/Dockerfile" \
    --tag hadoop-testing/hadoop-master${INDEX}:${PROJECT_VERSION} \
    "${SELF_DIR}/hadoop-master${INDEX}" $2

  rm -rf hadoop-master${INDEX}/download/*
}

build_hadoop_master_image 1 "$@"

function build_hadoop_worker_image() {
  local INDEX=$1
  mkdir -p hadoop-worker${INDEX}/download
  if [ $(uname -m) = "arm64" ] || [ $(uname -m) = "aarch64" ]; then HADOOP_TAR_NAME=hadoop-${HADOOP_VERSION}-aarch64; else HADOOP_TAR_NAME=hadoop-${HADOOP_VERSION}; fi
  $CP download/${HADOOP_TAR_NAME}.tar.gz hadoop-worker${INDEX}/download/hadoop-${HADOOP_VERSION}.tar.gz
  {% if spark_enabled %}
  $CP download/spark-${SPARK_VERSION}-bin-{{ spark_custom_name }}.tgz hadoop-worker${INDEX}/download/spark-${SPARK_VERSION}-bin-{{ spark_custom_name }}.tgz
  {% endif %}
  {% if trino_enabled %}
  $CP download/trino-server-${TRINO_VERSION}.tar.gz hadoop-worker${INDEX}/download/trino-server-${TRINO_VERSION}.tar.gz
  {% endif %}
  {% if spark_enabled %}
  tar -xzf hadoop-worker${INDEX}/download/spark-${SPARK_VERSION}-bin-{{ spark_custom_name }}.tgz -C hadoop-worker${INDEX}/download spark-${SPARK_VERSION}-bin-{{ spark_custom_name }}/yarn
  {% endif %}
  docker build \
    --build-arg PROJECT_VERSION=${PROJECT_VERSION} \
    --build-arg HADOOP_VERSION=${HADOOP_VERSION} \
    --build-arg SPARK_VERSION=${SPARK_VERSION} \
    --build-arg TRINO_VERSION=${TRINO_VERSION} \
    --file "${SELF_DIR}/hadoop-worker${INDEX}/Dockerfile" \
    --tag hadoop-testing/hadoop-worker${INDEX}:${PROJECT_VERSION} \
    "${SELF_DIR}/hadoop-worker${INDEX}" $2

  rm -rf hadoop-worker${INDEX}/download/*
}

build_hadoop_worker_image 1 "$@"
build_hadoop_worker_image 2 "$@"
build_hadoop_worker_image 3 "$@"
